%:
\documentclass[11pt]{article} 	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage[a4paper, total={7.5in, 10.5in}]{geometry}
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{diagbox}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{pythonhighlight}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\theoremstyle{definition}
\newtheorem*{defn}{Definition}
\newtheorem*{prop}{Proposition}
\newtheorem*{example}{Example}
\newtheorem*{thm}{Theorem}
\newtheorem*{corol}{Corollary}
\newtheorem{ex}{Exercise}[section]
{\theoremstyle{plain}
\newtheorem*{rmk}{Remark}
\newtheorem*{rmks}{Remarks}
\newtheorem*{lt}{Last time}
}
\newtheorem*{lem}{Lemma}
\usepackage{color}
\usepackage{CJK}
\title{Vision: A Programming Approach (work in progress)}
\author{Xiyu Zhai, Jiang Xin, Jian Qian, Haochuan Li, Xiao Ma, Sasha Rakhlin, Piotr Indyk}
\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle
\abstract{
Traditional feature engineering has been largely superseded by deep learning in many pattern recognition tasks in computer vision. However, tensor-based deep learning is still far from ideal. In fact, the perfect pattern recognition program that is accuracte, efficient, robust and explainable is not within its expressive power.

We invent a new generalized feature engineering approach allow us to create models close to optimal than deep learning in all aspects, accuracy, robustness, efficiency, explainability, maintainability. What one need for the new approach are only CPUs on one personal computer, zero GPUs, even for ImageNet. The core differences from traditional feature engineerrinig are  a more appropriate geometric understanding for image pattern recognition and a creation of a new programming language called Husky.

Accumulative ...

Experiments show that it's as accurate as typical neural networks in MNIST and ImageNet, with inference 20 times faster for MNIST and 100 times faster for ImageNet. The approach is applicable for most tasks in computer vision. Accuracy is as ..., explainability is ..., robustness is ...

The programming language is even applicable for other AI domains, which will be the focus of future research.
}

\mbox{}

\textit{
	This is work in progress and the list of authors is not exhaustive. It's open for collaboration. New collaborators will be added from time to time.}

\textit{It dates back to more than five years ago, when Xiyu Zhai considers theoretically how to do computer vision. It has evolved into a mixture of theory, programming language, computer vision, system and machine learning.}

\textit{The project already involves a huge amount of engineering, i.e. more than 2 years of a single PhD's dedicated work (without weekends of course). Yet another year of collaboration of a big team is needed for its completion. This draft is put on arXiv so that the project has a central reference. It will be frequently updated as we obtain new results. Also every member's contribution will be faithfully recorded.
}

\clearpage

\tableofcontents

\clearpage


\section{Introduction}

The field of computer vision has witnessed a lot of success primarily powered by deep learning. But what we get is still far from ideal.

The argument is as follows: there exists statistical and computational difficulty in prediction in pattern recognition tasks in computer vision, but deep learning attempts to address both classes of problems using only statistical function fitting.


So inevitably, deep learning leads to overparametrized model, slow to predict and costly to train and demand a lot of data and is not robust due to not incorporating domain specific data.

For analysis of MNIST, see later sections.

\section{Related Work}

\subsection{Traditional Feature Engineering}

\subsection{Bayesian}

The Bayesian is roughly like that, one has 

\subsection{Deep Learning}

\subsection{Deep Learning Acceleration}



\section{Motivation}

\subsection{Inference Efficiency}

\subsection{Robustness}

\subsection{Interpretability}

\subsection{Development Cost}

\subsection{Labeling Cost}

\subsection{Maintanence}

\subsection{A Lack of Scientific Approach}

Deep learning is an amazing, but it's anything but scientific, which is well known since the very beginning.

We're not against usage of deep learning, but it's ridiculous that it's the only thing that works.

Dominance by a non-scientific approach makes the whole field looks like the middle age.

Ideally there should be a spectrum of methods, from scientific but slow to implement to non-scientific but quick to make demo. We've discovered one end of the spectrum, it's time to discover the other end.




\section{Computational Difficulties in Prediction}

The typical setup of machine learning theory states everything in mathematical terms.
However, for many machine learning problem, especially those involved with pattern recognition, even the prediction(inference) involves nontrivial computation.

So the difficulty is twofold: statistical and computational.

What traditional machine learning and deep learning do is to solve problems of two different nature using statistical function fitting.
This works in practice, but the model is slow, large, not robust, inexplainable, costly to train.

We claim there are far better approaches.

This section describe theoretically a new paradigm.
We will explain how we reformulate machine learning setup in a way so that computational structure is captured.
We will also present our solutions to the newly formulated problems, which combines the good of both the programming world and the machine learning world.

The following sections will be about how it works in practice.


The typical setup of machine learning is like:

first you have an input space $\mathcal{X}$ and output space $\mathcal{Y}$, and a set of functions $\mathcal{H}$ from $\mathcal{X}$ to $\mathcal{Y}$.
There is an unknown function $f_0:\mathcal{X}\to \mathcal{Y}$.
There is a distribution $\mathcal{P}$ over $\mathcal{X}$, and i.i.d samples $x_i$ with $y_i=f_0(x_i)$ according to $\mathcal{P}$.
Then the problem is to choose an element in  $\mathcal{H}$ that best approximates this unknown function  $f_0$.

The problem with this formulation is that everything is stated in mathematically terms without mentioning its computational information.

 In many cases, we can choose a very concise $\mathcal{H}$ containing the ground truth, however functions in $\mathcal{H}$ might not be a function easy to compute, i.e. for $f\in \mathcal{H}$,
 it might take significant to compute for a given $x$ the value of $f(x)$.

 One possibility is that $f(x)$ could be of NP-form. Suppose that $\Gamma$ is a certificate space, and $\mathfrak{s}: \mathcal{X}\times \mathcal{Y}\times \Gamma\to \mathbb{R}$ is a score function that is reasonably easy to compute, with $\mathfrak{s}(x,y;\gamma)$ representing how credible $x$, and suppose that

 \begin{equation}
 	f(x)=\mathop{\text{argmax}}\limits_{y\in \mathcal{Y}}\sup_{\gamma\in \Gamma}\mathfrak{s}(x, y; \gamma),
 \end{equation}

 then $f(x)$ might be hard to compute.

 We claim this characterizes exactly the MNIST dataset.

\begin{example}[MNIST] Here we describe briefly a function in mathematical terms which we believe is the ground truth for the MNIST dataset. Details can be seen in appendix.

We take the convention that the fill of the digits is white and the background is black.

The image is represented by a $[0,1]$-valued $28\times 28$ matrix $I=(I_{ij})_{0\le i\le 27, 0\le j\le 27}$.

\begin{itemize}
	\item \textbf{digit one of the simplest kind}, which constitutes 95\% of all images of digit one.

A typical image looks like this:

[an image here]

Think about how it's drawn. The person when writing down a digit one like this has an ideal version in mind, a straight line that is almost vertical. So take $\Gamma_1$ to be the set of straight lines with slopes satisfying some easy constraint. Then we should define $\mathfrak{s}(x;\gamma)$ for $\gamma\in \Gamma_1$ such that
\begin{itemize}
	\item for "most" points over $\gamma$, it's surrounded by white pixels;
	\item for "most" non-white pixels, it's away from $\gamma$.
\end{itemize}

One choice could be
\begin{equation}
	\mathfrak{s}(x; \gamma)=a_1\int_{0}^1  \max_{(i,j)\in [27]\times[27]}1_{\|\gamma(t) - (i,j)\|_2 < \epsilon} I_{ij}dt
	-
	a_2\sum_{(i,j)\in [27]\times[27]} 1_{I_{ij}< 0.5}\text{dist}((i,j), \gamma)
\end{equation}

where $\epsilon$ is an appropriate small number and $a_1,a_2>0$ are appropriate coefficients.

In fact the function applies when $\gamma$ is any path, not necessarily a straight line. Formally it is defined over the path space (without basepoint) $\Gamma= M([0,1], [0,1]^2)$.

The dimensionality of the configuration space is 6, which can possibly be reduced to 5.

\item \textbf{digit seven of the simplest kind}.

A typical image looks like this:

[an image here]

Here we consider all continuous $\gamma:[0,2]\to [0,1]^2$ such that $\gamma|_{[0,1]}, \gamma|_{[1,2]}$ are straight line segments. Additionally, there should be some constraint on the positions of $\gamma(0), \gamma(1), \gamma(2)$ such that $\overline{\gamma(0)\gamma(1)}$ is very close to being horizontal, and $\overline{\gamma(1)\gamma(2)}$ should be roughly vertical downward.

The score function $\mathfrak{s}$ is the same.

The dimensionality of the configuration space is 6, which can actually be reduced to 5.

\item \textbf{digit zero}.

We consider smooth curves $\gamma:[0,L]\to [0,1]^2$ with arc length parametrization such that the mean curvature is always nonnegative, i.e.
\begin{equation}
	\|\gamma'(t)\|\equiv 1
\end{equation}
and
\begin{equation}
	\gamma''(t)\times \gamma'(t) \ge 0
\end{equation}

Additionally, we require that $\gamma(0)$ is very close to $\gamma(L)$.

We should also require that $\gamma$ is nondegenerate, which can be characterized by isoperimetric inequality.

All these $\gamma$ form $\Gamma_0$.

And we still use the same score funtion $\mathfrak{s}$.

\item \textbf{general case}. Fix a graph $G=(V,E)$. Give it a natural topological structure and identify each $e$ with $[0,1]$. For each $e\in E$, we give assign a $\sigma_e$ which is one of the following classes of curves:

\begin{itemize}
	\item nonconvex but not straight
	\item nonconcave but not straight
	\item straight.
\end{itemize}

We define the total space as
\begin{equation}
	\Omega_G = \left\{\gamma\in M(G, [0,1]^2): \forall e\in E, \gamma|_e\in \sigma_e\right\}.
\end{equation}

Then the configuration space $\Gamma_G$ is a subset of $\Omega_G$ such that it is given by a boolean function $s$ in the sense that
\begin{equation}
	1_{\Gamma_G}(\gamma)=s((\gamma(v))_{v\in V},((\gamma|_e'(0), \gamma|_e'(1), \text{dist}(\gamma|_e,\overline{\gamma|_e(0)\gamma|_e(1)})))_{e\in E})
\end{equation}
\end{itemize}
\end{example}

\section{A New Paradigm for Pattern Recognition }
\subsection{Pattern Recognition VS Machine Learning}

\subsection{Rethink Hypothesis Space}

The ultimate goal for pattern recognition is to get an accurate efficient program which is inherently discrete.  The search space of program is computational expensive to explore.  For example, we can consider the space of all programs that can be described by N characters. The number of samples we need is proportional to N. But the size of the space to search is exponentially large making it impossible to directly search in space.  As a result, we have to find some rules to narrow the search space so that we can find an approximate solution to the problem in a limited time. 

Machine learning theory narrows this search space down to a hypothesis space which is typically parametrized by a high dimensional vector space.  The parametrization must be "smooth" enough so that end-to-end optimization is possible.  So the hypothesis space is continuous.

In fact, what traditional machine learning and deep learning does is to use a small subset that is approximately smooth.  This work in many cases if one only cares about accuracy and has access to a lot of data and computing resources.  




\subsection{Feature Definition}

\subsection{Rethink Training}



\section{Domain Specifics for Image Classification}

\subsection{Line Segment Sketching and Convex/Convex Components}

\subsection{Three Families of Features}

\subsection{Composition of Features}



\section{Unprecedented Engineering Challenges}

\subsection{Coding}

The primary challenges are

\begin{itemize}
	\item efficient geometric algorithms are not straightforward to code, prone to bug and corner cases
	\item need to write system level code
	\item need to write high level code for feature construction, possibly with automation, and even automation of automation
	\item visualization of features, possibly over samples within a certain branch
	\item fast iteration
	\item large dataset
	\item a strong type system that expresses feature, and feature composition
\end{itemize}

Current languages like python, C++, Rust are far from being able to satisfies the needs, which is one of the primary reasons why the paradigm hasn't been discovered yet.

It's obvious that a new language is needed. It took two years for the Husky programming language to be created for this purpose.

\subsection{Static Analysis}

\subsection{Compilation and Evaluation}

\subsection{Debugging}


\section{Design of the Husky Programming Language}

This deserves a separate long paper. But for the sake of logic completeness, we give a brief overview here.

The language itself will be general purpose. But here we focus on its usage in computer vision.

\subsection{Syntax}

The syntax is pythonic.

One can define a feature by
\begin{lstlisting}[language=Python]
def feature_a:
    1
\end{lstlisting}

or equivalently,

\begin{lstlisting}[language=Python]
feature_a = 1.
\end{lstlisting}

Another feature can be defined based on it,

\begin{lstlisting}[language=Python]
def feature_b:
    a_feature + 1
\end{lstlisting}

or equivalently

\begin{lstlisting}[language=Python]
feature_b = a_feature + 1
\end{lstlisting}

This will make the value of feature b on any input equal to the value of feature a plus 1.

\subsection{Type System}

The core invention is ascension.

A type system can be seen as a category $\mathcal{C}$. Consider a functor $\mathcal{F}$ from $\mathcal{C}$ to another category $\mathcal{D}$,
the functor also gives mappings from $Mor(X,Y)$ to $Mor(\mathcal{F}(X), \mathcal{F}(Y))$ for 

An ascension is a collection of functors like these.

\subsection{Compiler and Evaluator}

\subsection{Debugger}


\section{Results on MNIST}

\subsection{Overview}

\subsection{Digit One}


\section{Results on ImageNet}

\subsection{Overview}

\subsection{Husky Dog}

\subsection{Samoyed Dog}

\subsection{Piano}

\section{Advantages over Pure Deep Learning}

\subsection{Extensible}

In deep learning, to test a new idea, one typically has to rerun the experiments over a large datasets using multiple GPUs, waiting for days the new results.

But in husky, one can try ideas instantaneously, based on previous people's result.

The difference is because in deep learning, everything is mixed together, a change in any parameter leads to uncontrollable change in prediction for any input. Everything is fully inexplainable.

But in Husky, a model is no different from a computer software, which is divided into packages, modules, features and functions. It's easy to isolate the difficulty, and separate things of different natures. Everything is fully explainable.

\subsection{}


\section{Future}

\subsection{Other Tasks in Computer Vision}

One thing great about Husky is that it inherently does segmentation and detection even during classification tasks. So it would be easy to do them.

The sketching algorithm generalizes easily to 3D, resulting in fundamentally novel shape analysis. So 3D tasks are doable too.

\subsection{Go Beyond Computer Vision}

\appendix

\section{Related Work Details}

\section{Image Classification Domain Specific Details}

\subsection{Line Sketch Algorithms}


\section{Language Design Details}

\section{Mnist Details}

\subsection{Overview}


\subsection{Digit One}

\subsection{Digit Two}

\subsection{Digit Three}

\subsection{Digit Four}

\subsection{Digit Five}

\section{ImageNet Details}

\subsection{Overview}

\subsection{Husky}

\subsection{Piano}

\subsection{Dog}

\subsection{Cat}

\subsection{Object}

\section{Contributions}

This section is for listing individual contributions.

Xiyu Zhai's contribution can be summarized as
\begin{itemize}
	\item line sketching algorithm and convex concave components and proof of its stability
	\item theoretical formulation of the computational difficulty in prediction
	\item the perspective of field vs particle
	\item type theory for features
	\item lazy feature computation
	\item most of the design, implementation and maintenance of the Husky programming language
	\item mnist, imagenet experiments
	\item realization that decision list is better than decision tree
\end{itemize}

Jiang Xin's contribution can be summarized as
\begin{itemize}
	\item help with mnist, imagenet experiments
\end{itemize}

Jian Qian's contribution can be summarized as
\begin{itemize}
	\item clarify theoretical formulation
\end{itemize}

Haochuan Li's contribution can be summarized as
\begin{itemize}
	\item clarify theoretical formulation
\end{itemize}

Xiao Ma: TBA.

Sasha Rakhlin provides insights from learning theory. Fundamental machine learning concepts still apply.

Piotr Indyk provides insights from applied algorithms. The computation is inspired by LSH in the early days.




\end{document}