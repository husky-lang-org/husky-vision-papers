%:
\documentclass[11pt]{article} 	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage[a4paper, total={7.5in, 10.5in}]{geometry}
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{diagbox}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\theoremstyle{definition}
\newtheorem*{defn}{Definition}
\newtheorem*{prop}{Proposition}
\newtheorem*{example}{Example}
\newtheorem*{thm}{Theorem}
\newtheorem*{corol}{Corollary}
\newtheorem{ex}{Exercise}[section]
{\theoremstyle{plain}
\newtheorem*{rmk}{Remark}
\newtheorem*{rmks}{Remarks}
\newtheorem*{lt}{Last time}
}
\newtheorem*{lem}{Lemma}
\usepackage{color}
\usepackage{CJK}
\title{Vision: A Programming Approach}
\author{Xiyu Zhai, Jiang Xin, Jian Qian, Haochuan Li, Xiao Ma, Sasha Rakhlin, Piotr Indyk}
\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle
\tableofcontents
\abstract{
	We introduce a fundamentally novel computer vision methodology that can explore possibilities beyond traditional computer vision and end-to-end deep learning. Using just CPUs, it can develop prediction programs with equal or better accuracy,  efficiency and robustness, and full explainability. The effective computation needed in training is basically ignorable as opposed to that of deep learning. However, the paradigm is difficult to implement using existing programming languages, we invented a new general purpose programming language called Husky for this purpose.
}

\section{Introduction}

The field of computer vision has witnessed a lot of success primarily powered by deep learning. But what we get is still far from ideal.

The argument is as follows: there exists statistical and computational difficulty in prediction in pattern recognition tasks in computer vision, but deep learning attempts to address both classes of problems using only statistical function fitting.


So inevitably, deep learning leads to overparametrized model, slow to predict and costly to train and demand a lot of data and is not robust due to not incorporating domain specific data.

For analysis of MNIST, see later sections.

\section{Notions}

We shall use pattern recognition to mean the prediction process.

What we do is not machine learning, but a more generalized feature engineering which includes machine learning as a subset.

\section{Motivation}

TODO


\section{A New Paradigm}

The typical setup of machine learning theory states everything in mathematical terms.
However, for many machine learning problem, especially those involved with pattern recognition, even the prediction(inference) involves nontrivial computation.

So the difficulty is twofold: statistical and computational.

What traditional machine learning and deep learning do is to solve problems of two different nature using statistical function fitting.
This works in practice, but the model is slow, large, not robust, inexplainable, costly to train.

We claim there are far better approaches.

This section describe theoretically a new paradigm.
We will explain how we reformulate machine learning setup in a way so that computational structure is captured.
We will also present our solutions to the newly formulated problems, which combines the good of both the programming world and the machine learning world.

The following sections will be about how it works in practice.

\subsection{Computational Difficulties in Prediction}

The typical setup of machine learning is like:

first you have an input space $\mathcal{X}$ and output space $\mathcal{Y}$, and a set of functions $\mathcal{H}$ from $\mathcal{X}$ to $\mathcal{Y}$.
There is an unknown function $f_0:\mathcal{X}\to \mathcal{Y}$.
There is a distribution $\mathcal{P}$ over $\mathcal{X}$, and i.i.d samples $x_i$ with $y_i=f_0(x_i)$ according to $\mathcal{P}$.
Then the problem is to choose an element in  $\mathcal{H}$ that best approximates this unknown function  $f_0$.

The problem with this formulation is that everything is stated in mathematically terms without mentioning its computational information.

 In many cases, we can choose a very concise $\mathcal{H}$ containing the ground truth, however functions in $\mathcal{H}$ might not be a function easy to compute, i.e. for $f\in \mathcal{H}$,
 it might take significant to compute for a given $x$ the value of $f(x)$.

 One possibility is that $f(x)$ could be of NP-form. Suppose that $\Gamma$ is a certificate space, and $\mathfrak{s}: \mathcal{X}\times \mathcal{Y}\times \Gamma\to \mathbb{R}$ is a score function that is reasonably easy to compute, with $\mathfrak{s}(x,y;\gamma)$ representing how credible $x$, and suppose that

 \begin{equation}
 	f(x)=\mathop{\text{argmax}}\limits_{y\in \mathcal{Y}}\sup_{\gamma\in \Gamma}\mathfrak{s}(x, y; \gamma),
 \end{equation}

 then $f(x)$ might be hard to compute.

 We claim this characterizes exactly the MNIST dataset.

\begin{example}[MNIST] Here we describe briefly a function in mathematical terms which we believe is the ground truth for the MNIST dataset. Details can be seen in appendix.

We take the convention that the fill of the digits is white and the background is black.

The image is represented by a $[0,1]$-valued $28\times 28$ matrix $I=(I_{ij})_{0\le i\le 27, 0\le j\le 27}$.

\begin{itemize}
	\item  let's first consider how to describe an image of \textbf{digit one of the simplest kind}, which constitutes 95\% of all images of digit one.

A typical image looks like this:

[an image here]

Think about how it's drawn. The person when writing down a digit one like this has an ideal version in mind, a straight line that is almost vertical. So take $\Gamma_1$ to be the set of straight lines with slopes satisfying some easy constraint. Then we should define $\mathfrak{s}(x;\gamma)$ for $\gamma\in \Gamma_1$ such that
\begin{itemize}
	\item for "most" points over $\gamma$, it's surrounded by white pixels;
	\item for "most" non-white pixels, it's away from $\gamma$.
\end{itemize}

One choice could be
\begin{equation}
	\mathfrak{s}(x; \gamma)=a_1\int_{0}^1  \max_{(i,j)\in [27]\times[27]}1_{\|\gamma(t) - (i,j)\|_2 < \epsilon} I_{ij}dt
	-
	a_2\sum_{(i,j)\in [27]\times[27]} 1_{I_{ij}< 0.5}\text{dist}((i,j), \gamma)
\end{equation}

where $\epsilon$ is an appropriate small number and $a_1,a_2>0$ are appropriate coefficients.

In fact the function applies when $\gamma$ is any path, not necessarily a straight line. Formally it is defined over the path space (without basepoint) $\Gamma= M([0,1], [0,1]^2)$.

The dimensionality of the configuration space is 6, which can possibly be reduced to 5.

\item now let's consider a harder case, \textbf{digit seven of the simplest kind}.

A typical image looks like this:

[an image here]

Here we consider all continuous $\gamma:[0,2]\to [0,1]^2$ such that $\gamma|_{[0,1]}, \gamma|_{[1,2]}$ are straight line segments. Additionally, there should be some constraint on the positions of $\gamma(0), \gamma(1), \gamma(2)$ such that $\overline{\gamma(0)\gamma(1)}$ is very close to being horizontal, and $\overline{\gamma(1)\gamma(2)}$ should be roughly vertical downward.

The score function $\mathfrak{s}$ is the same.

The dimensionality of the configuration space is 6, which can actually be reduced to 5.

\item now let's consider a much harder case, \textbf{digit zero}.

We consider smooth curves $\gamma:[0,L]\to [0,1]^2$ with arc length parametrization such that the mean curvature is always nonnegative, i.e.
\begin{equation}
	\|\gamma'(t)\|\equiv 1
\end{equation}
and
\begin{equation}
	\gamma''(t)\times \gamma'(t) \ge 0
\end{equation}

Additionally, we require that $\gamma(0)$ is very close to $\gamma(L)$.

We should also require that $\gamma$ is nondegenerate, which can be characterized by isoperimetric inequality.

All these $\gamma$ form $\Gamma_0$.

And we still use the same score funtion $\mathfrak{s}$.

\item \textbf{general case}. Fix a graph $G=(V,E)$. Give it a natural topological structure and identify each $e$ with $[0,1]$. For each $e\in E$, we give assign a $\sigma_e$ which is one of the following classes of curves:

\begin{itemize}
	\item nonconvex but not straight
	\item nonconcave but not straight
	\item straight.
\end{itemize}

We define the total space as
\begin{equation}
	\Omega_G = \left\{\gamma\in M(G, [0,1]^2): \forall e\in E, \gamma|_e\in \sigma_e\right\}.
\end{equation}

Then the configuration space $\Gamma_G$ is a subset of $\Omega_G$ such that it is given by a boolean function $s$ in the sense that
\begin{equation}
	1_{\Gamma_G}(\gamma)=s((\gamma(v))_{v\in V},((\gamma|_e'(0), \gamma|_e'(1), \text{dist}(\gamma|_e,\overline{\gamma|_e(0)\gamma|_e(1)})))_{e\in E})
\end{equation}
\end{itemize}
\end{example}

\subsection{Rethinking Hypothesis Space}

\subsection{Rethinking Training}

\subsection{Challenge of the New Paradigm}

The primary challenges are

\begin{itemize}
	\item efficient geometric algorithms are not straightforward to code, prone to bug and corner cases
	\item need to write system level code
	\item need to write high level code for feature construction, possibly with automation, and even automation of automation
	\item visualization of features, possibly over samples within a certain branch
	\item fast iteration
	\item large dataset
	\item a strong type system that expresses feature, and feature composition
\end{itemize}

Current languages like python, C++, Rust are far from being able to satisfies the needs, which is one of the primary reasons why the paradigm hasn't been discovered yet.

It's obvious that a new language is needed. It took two years for the Husky programming language to be created for this purpose.

\section{Design of the Husky Programming Language}

This deserves a separate long paper. But for the sake of logic completeness, we give a brief overview here.

The language itself will be general purpose. But here we focus on its usage in computer vision.

\subsection{syntax}

The syntax is pythonic.

One can define a feature by
\begin{lstlisting}[language=Python, caption=Python example]
def a_feature -> i32:
	1
\end{lstlisting}

\subsection{type system}

\subsection{compilation and evaluation}

\subsection{debugger}


\section{MNIST}

\section{ImageNet}



\section{Advantages over Pure Deep Learning}

\subsection{Extensible}

In deep learning, to test a new idea, one typically has to rerun the experiments over a large datasets using multiple GPUs, waiting for days the new results.

But in husky, one can try ideas instantaneously, based on previous people's result.

The difference is because in deep learning, everything is mixed together, a change in any parameter leads to uncontrollable change in prediction for any input. Everything is fully inexplainable.

But in Husky, a model is no different from a computer software, which is divided into packages, modules, features and functions. It's easy to isolate the difficulty, and separate things of different natures. Everything is fully explainable.

\subsection{}


\section{Future}

\subsection{Other Tasks in Computer Vision}

One thing great about Husky is that it inherently does segmentation and detection even during classification tasks. So it would be easy to do them.

The sketching algorithm generalizes easily to 3D, resulting in fundamentally novel shape analysis. So 3D tasks are doable too.

\subsection{Go Beyond Computer Vision}

\appendix

\section{Proof Details}

\section{Language Design Details}

\section{Mnist Details}

\section{ImageNet Details}

\section{Contributions}

This section is for listing individual contributions.

Xiyu Zhai's contribution can be summarized as
\begin{itemize}
	\item line sketching algorithm and convex concave components and proof of its stability
	\item theoretical formulation of the computational difficulty in prediction
	\item the perspective of field vs particle
	\item type theory for features
	\item lazy feature computation
	\item most of the design, implementation and maintenance of the Husky programming language
	\item mnist, imagenet experiments
	\item realization that decision list is better than decision tree
\end{itemize}

Jiang Xin's contribution can be summarized as
\begin{itemize}
	\item help with mnist, imagenet experiments
\end{itemize}

Jian Qian's contribution can be summarized as
\begin{itemize}
	\item clarify theoretical formulation
\end{itemize}

Haochuan Li's contribution can be summarized as
\begin{itemize}
	\item clarify theoretical formulation
\end{itemize}

Xiao Ma: TBA.

Sasha Rakhlin provides insights from learning theory. Fundamental machine learning concepts still apply.

Piotr Indyk provides insights from applied algorithms. The computation is inspired by LSH in the early days.




\end{document}